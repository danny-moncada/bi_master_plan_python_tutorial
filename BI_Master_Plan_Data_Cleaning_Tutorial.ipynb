{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning Tutorial: Cleaning Data Science Job Postings Dataset\n",
    "\n",
    "In this tutorial, we will walk through the process of cleaning the \"Data Science Job Postings\" dataset. The dataset contains information about job postings related to data science scraped from Glassdoor.\n",
    "\n",
    "### Step 1: Loading the Dataset\n",
    "\n",
    "First, let's start by loading the dataset into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we will be using in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the file using the \"read_csv\" function in the pandas library and store in a dataframe called `ds_df` (data science data frame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = pd.read_csv('data files/Uncleaned_DS_jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first five rows of the data set using the head() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Description\\n\\nThe Senior Data Scientist is re...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Healthfirst\\n3.1</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1993</td>\n",
       "      <td>Nonprofit Organization</td>\n",
       "      <td>Insurance Carriers</td>\n",
       "      <td>Insurance</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>EmblemHealth, UnitedHealth Group, Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Secure our Nation, Ignite your Future\\n\\nJoin ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>ManTech\\n4.2</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Herndon, VA</td>\n",
       "      <td>5001 to 10000 employees</td>\n",
       "      <td>1968</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$1 to $2 billion (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Overview\\n\\n\\nAnalysis Group is one of the lar...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Analysis Group\\n3.8</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1981</td>\n",
       "      <td>Private Practice / Firm</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$100 to $500 million (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>JOB DESCRIPTION:\\n\\nDo you have a passion for ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>INFICON\\n3.5</td>\n",
       "      <td>Newton, MA</td>\n",
       "      <td>Bad Ragaz, Switzerland</td>\n",
       "      <td>501 to 1000 employees</td>\n",
       "      <td>2000</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Electrical &amp; Electronic Manufacturing</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>$100 to $500 million (USD)</td>\n",
       "      <td>MKS Instruments, Pfeiffer Vacuum, Agilent Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Data Scientist\\nAffinity Solutions / Marketing...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Affinity Solutions\\n2.9</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>51 to 200 employees</td>\n",
       "      <td>1998</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>Commerce Signals, Cardlytics, Yodlee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          Job Title               Salary Estimate  \\\n",
       "0      0  Sr Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "1      1     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "2      2     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "3      3     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "4      4     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  \\\n",
       "0  Description\\n\\nThe Senior Data Scientist is re...     3.1   \n",
       "1  Secure our Nation, Ignite your Future\\n\\nJoin ...     4.2   \n",
       "2  Overview\\n\\n\\nAnalysis Group is one of the lar...     3.8   \n",
       "3  JOB DESCRIPTION:\\n\\nDo you have a passion for ...     3.5   \n",
       "4  Data Scientist\\nAffinity Solutions / Marketing...     2.9   \n",
       "\n",
       "              Company Name       Location            Headquarters  \\\n",
       "0         Healthfirst\\n3.1   New York, NY            New York, NY   \n",
       "1             ManTech\\n4.2  Chantilly, VA             Herndon, VA   \n",
       "2      Analysis Group\\n3.8     Boston, MA              Boston, MA   \n",
       "3             INFICON\\n3.5     Newton, MA  Bad Ragaz, Switzerland   \n",
       "4  Affinity Solutions\\n2.9   New York, NY            New York, NY   \n",
       "\n",
       "                      Size  Founded        Type of ownership  \\\n",
       "0   1001 to 5000 employees     1993   Nonprofit Organization   \n",
       "1  5001 to 10000 employees     1968         Company - Public   \n",
       "2   1001 to 5000 employees     1981  Private Practice / Firm   \n",
       "3    501 to 1000 employees     2000         Company - Public   \n",
       "4      51 to 200 employees     1998        Company - Private   \n",
       "\n",
       "                                Industry             Sector  \\\n",
       "0                     Insurance Carriers          Insurance   \n",
       "1                 Research & Development  Business Services   \n",
       "2                             Consulting  Business Services   \n",
       "3  Electrical & Electronic Manufacturing      Manufacturing   \n",
       "4                Advertising & Marketing  Business Services   \n",
       "\n",
       "                      Revenue  \\\n",
       "0    Unknown / Non-Applicable   \n",
       "1      $1 to $2 billion (USD)   \n",
       "2  $100 to $500 million (USD)   \n",
       "3  $100 to $500 million (USD)   \n",
       "4    Unknown / Non-Applicable   \n",
       "\n",
       "                                         Competitors  \n",
       "0            EmblemHealth, UnitedHealth Group, Aetna  \n",
       "1                                                 -1  \n",
       "2                                                 -1  \n",
       "3  MKS Instruments, Pfeiffer Vacuum, Agilent Tech...  \n",
       "4               Commerce Signals, Cardlytics, Yodlee  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an `index` column which isn't really of any use to us so let's remove this before continuing on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Exploring the Dataset\n",
    "\n",
    "Before we begin cleaning the data, it's important to understand its structure and identify any potential issues. Let us explore the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed an `index` column, which will not be of any use to us in our analysis; let us remove this before continuing on.  We can do this with the df.drop() function in pandas; we use `inplace=True` so that the dataframe will save this change for the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.drop('index', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimensions of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Dimensions: (672, 14)\n"
     ]
    }
   ],
   "source": [
    "print('Dataset Dimensions:', ds_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the column names to see what kind of data we can analyze.  We can also sort them using the `sort_values()` function so it is a little easier to see what is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column Names:\n",
      "\n",
      "Company Name\n",
      "Competitors\n",
      "Founded\n",
      "Headquarters\n",
      "Industry\n",
      "Job Description\n",
      "Job Title\n",
      "Location\n",
      "Rating\n",
      "Revenue\n",
      "Salary Estimate\n",
      "Sector\n",
      "Size\n",
      "Type of ownership\n"
     ]
    }
   ],
   "source": [
    "print('Column Names:\\n')\n",
    "for i in ds_df.columns.sort_values():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Company Name**: Company that posted the job\n",
    "* **Competitors**: Companies that are direct competitors\n",
    "* **Founded**: Year the company was founded or established\n",
    "* **Headquarters**: Location of company headquarters\n",
    "* **Industry**: Industry of the company\n",
    "* **Job Description**: The full job description for the job posted\n",
    "* **Job Title**: Title of the position in the job posting\n",
    "* **Location**: Job location\n",
    "* **Rating**: Rating of that post\n",
    "* **Revenue**: Total revenue for the company\n",
    "* **Salary Estimate**: Salary range for that particular job\n",
    "* **Sector**: Sector of the company\n",
    "* **Size**: Total number of employees at the company\n",
    "* **Type of ownership**: Public/private/non-profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an idea of what kind of data we can start analyzing, let us get a better sense of the data completeness and what kind of data types are in the data set.  We have a couple of different ways to do this, but let us use an extremely helpful dataframe function `df.info()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 672 entries, 0 to 671\n",
      "Data columns (total 14 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Job Title          672 non-null    object \n",
      " 1   Salary Estimate    672 non-null    object \n",
      " 2   Job Description    672 non-null    object \n",
      " 3   Rating             672 non-null    float64\n",
      " 4   Company Name       672 non-null    object \n",
      " 5   Location           672 non-null    object \n",
      " 6   Headquarters       672 non-null    object \n",
      " 7   Size               672 non-null    object \n",
      " 8   Founded            672 non-null    int64  \n",
      " 9   Type of ownership  672 non-null    object \n",
      " 10  Industry           672 non-null    object \n",
      " 11  Sector             672 non-null    object \n",
      " 12  Revenue            672 non-null    object \n",
      " 13  Competitors        672 non-null    object \n",
      "dtypes: float64(1), int64(1), object(12)\n",
      "memory usage: 73.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(ds_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us a great deal about the data we'll be working with.  We see that are no null values anywhere in the data set, which is normally something that we would need to handle, either via extrapolation or removing those records.  Luckily for us, we do not need to worry about that with this data set so we will not cover that in this tutorial.*\n",
    "\n",
    "We see that, for the most part, the data types are `object` which in Python terms means they will be string values.  The **Rating** column is a float so there must be decimals in the ratings.  The **Founded** column is an integer which means that this is likely simply just the year the company was founded.\n",
    "\n",
    "What is unfortunate is that we do not have *when* the job posting was placed in this data set, so we cannot get a sense of how old this data is now.  Considering the data set was published in 2021, we can infer that it was scrapped around that time (and this is one of the challenges of getting publicly available data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If we did want to deal with missing values, there are a few handy functions that can be used.\n",
    "\n",
    "For example, we could drop columns with high missing value percentage by setting a threshold and using the `dropna()`\n",
    "\n",
    "`threshold = 0.75`\n",
    "\n",
    "`ds_df = ds_df.dropna(thresh=threshold * len(ds_df), axis=1)`\n",
    "\n",
    "Or we could fill in empty values with our own fix using the `fillna()` function.\n",
    "\n",
    "`ds_df['Sector'] = data['Sector'].fillna('Not Available')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Remove duplicate rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have to deal with missing values, let us see if there are any duplicate records and remove those. We can do this using the `df.duplicated()` function and summing to get the count of duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too many duplicates, but let us remove them by running `df.drop_duplicates()`.  We will also reset the index on the data frame since we're dropping rows and do not want to lose track of the new row count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape to see the new dataframe dimensions and confirm the thirteen duplicate rows are gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(659, 14)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Cleaning and Transforming Data\n",
    "\n",
    "Now, let's clean and transform the data to ensure consistency and uniformity.\n",
    "\n",
    "To start, let us look at the first five rows of data to see if there are any manipulations or clean up steps we should take before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Salary Estimate</th>\n",
       "      <th>Job Description</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Location</th>\n",
       "      <th>Headquarters</th>\n",
       "      <th>Size</th>\n",
       "      <th>Founded</th>\n",
       "      <th>Type of ownership</th>\n",
       "      <th>Industry</th>\n",
       "      <th>Sector</th>\n",
       "      <th>Revenue</th>\n",
       "      <th>Competitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sr Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Description\\n\\nThe Senior Data Scientist is re...</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Healthfirst\\n3.1</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1993</td>\n",
       "      <td>Nonprofit Organization</td>\n",
       "      <td>Insurance Carriers</td>\n",
       "      <td>Insurance</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>EmblemHealth, UnitedHealth Group, Aetna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Secure our Nation, Ignite your Future\\n\\nJoin ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>ManTech\\n4.2</td>\n",
       "      <td>Chantilly, VA</td>\n",
       "      <td>Herndon, VA</td>\n",
       "      <td>5001 to 10000 employees</td>\n",
       "      <td>1968</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Research &amp; Development</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$1 to $2 billion (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Overview\\n\\n\\nAnalysis Group is one of the lar...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>Analysis Group\\n3.8</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>Boston, MA</td>\n",
       "      <td>1001 to 5000 employees</td>\n",
       "      <td>1981</td>\n",
       "      <td>Private Practice / Firm</td>\n",
       "      <td>Consulting</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>$100 to $500 million (USD)</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>JOB DESCRIPTION:\\n\\nDo you have a passion for ...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>INFICON\\n3.5</td>\n",
       "      <td>Newton, MA</td>\n",
       "      <td>Bad Ragaz, Switzerland</td>\n",
       "      <td>501 to 1000 employees</td>\n",
       "      <td>2000</td>\n",
       "      <td>Company - Public</td>\n",
       "      <td>Electrical &amp; Electronic Manufacturing</td>\n",
       "      <td>Manufacturing</td>\n",
       "      <td>$100 to $500 million (USD)</td>\n",
       "      <td>MKS Instruments, Pfeiffer Vacuum, Agilent Tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>$137K-$171K (Glassdoor est.)</td>\n",
       "      <td>Data Scientist\\nAffinity Solutions / Marketing...</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Affinity Solutions\\n2.9</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>New York, NY</td>\n",
       "      <td>51 to 200 employees</td>\n",
       "      <td>1998</td>\n",
       "      <td>Company - Private</td>\n",
       "      <td>Advertising &amp; Marketing</td>\n",
       "      <td>Business Services</td>\n",
       "      <td>Unknown / Non-Applicable</td>\n",
       "      <td>Commerce Signals, Cardlytics, Yodlee</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Job Title               Salary Estimate  \\\n",
       "0  Sr Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "1     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "2     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "3     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "4     Data Scientist  $137K-$171K (Glassdoor est.)   \n",
       "\n",
       "                                     Job Description  Rating  \\\n",
       "0  Description\\n\\nThe Senior Data Scientist is re...     3.1   \n",
       "1  Secure our Nation, Ignite your Future\\n\\nJoin ...     4.2   \n",
       "2  Overview\\n\\n\\nAnalysis Group is one of the lar...     3.8   \n",
       "3  JOB DESCRIPTION:\\n\\nDo you have a passion for ...     3.5   \n",
       "4  Data Scientist\\nAffinity Solutions / Marketing...     2.9   \n",
       "\n",
       "              Company Name       Location            Headquarters  \\\n",
       "0         Healthfirst\\n3.1   New York, NY            New York, NY   \n",
       "1             ManTech\\n4.2  Chantilly, VA             Herndon, VA   \n",
       "2      Analysis Group\\n3.8     Boston, MA              Boston, MA   \n",
       "3             INFICON\\n3.5     Newton, MA  Bad Ragaz, Switzerland   \n",
       "4  Affinity Solutions\\n2.9   New York, NY            New York, NY   \n",
       "\n",
       "                      Size  Founded        Type of ownership  \\\n",
       "0   1001 to 5000 employees     1993   Nonprofit Organization   \n",
       "1  5001 to 10000 employees     1968         Company - Public   \n",
       "2   1001 to 5000 employees     1981  Private Practice / Firm   \n",
       "3    501 to 1000 employees     2000         Company - Public   \n",
       "4      51 to 200 employees     1998        Company - Private   \n",
       "\n",
       "                                Industry             Sector  \\\n",
       "0                     Insurance Carriers          Insurance   \n",
       "1                 Research & Development  Business Services   \n",
       "2                             Consulting  Business Services   \n",
       "3  Electrical & Electronic Manufacturing      Manufacturing   \n",
       "4                Advertising & Marketing  Business Services   \n",
       "\n",
       "                      Revenue  \\\n",
       "0    Unknown / Non-Applicable   \n",
       "1      $1 to $2 billion (USD)   \n",
       "2  $100 to $500 million (USD)   \n",
       "3  $100 to $500 million (USD)   \n",
       "4    Unknown / Non-Applicable   \n",
       "\n",
       "                                         Competitors  \n",
       "0            EmblemHealth, UnitedHealth Group, Aetna  \n",
       "1                                                 -1  \n",
       "2                                                 -1  \n",
       "3  MKS Instruments, Pfeiffer Vacuum, Agilent Tech...  \n",
       "4               Commerce Signals, Cardlytics, Yodlee  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few observations we can make from the first five rows in the data set.\n",
    "\n",
    "1.  The Salary Estimate column has a lot of extra characters that we can clean up, and we could format this in a better way to showcase a range.\n",
    "2.  The Company Name column has a newline `\\n` & the company rating following it (probably due to a parsing error); we should remove anything after the `\\n` so we only have the company name.\n",
    "3.  The Revenue column is also a bit of a mess, containing both revenue ranges and Unknowns so we're not completely done with N/As!\n",
    "4.  Finally, the Competitors column has some `-1`s; this is probably supposed to be an empty value and we'll take a look to see how many rows this affects before we decide what to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the company name since this will be a pretty straightforward fix.  We can start by just printing the column out to confirm the `\\n#.#` appears on every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               Healthfirst\\n3.1\n",
       "1                   ManTech\\n4.2\n",
       "2            Analysis Group\\n3.8\n",
       "3                   INFICON\\n3.5\n",
       "4        Affinity Solutions\\n2.9\n",
       "                 ...            \n",
       "667                TRANZACT\\n3.6\n",
       "668                         JKGT\n",
       "669                   AccessHope\n",
       "670    ChaTeck Incorporated\\n5.0\n",
       "671           1-800-Flowers\\n2.7\n",
       "Name: Company Name, Length: 659, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Company Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas comes with a variety of different ways to do this; we can use the `str.split()` method to pull apart each element in the Company Name column.  Then we can overwrite the original column by assigning it back to the column.\n",
    "\n",
    "`ds_df['Company Name'].str.split('\\n', expand=True)` We split on the \\n character, expand the columns so it splits them into two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Healthfirst</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ManTech</td>\n",
       "      <td>4.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Analysis Group</td>\n",
       "      <td>3.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INFICON</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Affinity Solutions</td>\n",
       "      <td>2.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>TRANZACT</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>JKGT</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>AccessHope</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>ChaTeck Incorporated</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>1-800-Flowers</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0     1\n",
       "0             Healthfirst   3.1\n",
       "1                 ManTech   4.2\n",
       "2          Analysis Group   3.8\n",
       "3                 INFICON   3.5\n",
       "4      Affinity Solutions   2.9\n",
       "..                    ...   ...\n",
       "667              TRANZACT   3.6\n",
       "668                  JKGT  None\n",
       "669            AccessHope  None\n",
       "670  ChaTeck Incorporated   5.0\n",
       "671         1-800-Flowers   2.7\n",
       "\n",
       "[659 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Company Name'].str.split('\\n', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we grab the company name by only selecting the first element, which in Python starts with `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Company Name'] = ds_df['Company Name'].str.split('\\n', expand=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all of our transformations, we want to print out the results of our work to confirm our code is performing the transformations as we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               Healthfirst\n",
       "1                   ManTech\n",
       "2            Analysis Group\n",
       "3                   INFICON\n",
       "4        Affinity Solutions\n",
       "               ...         \n",
       "667                TRANZACT\n",
       "668                    JKGT\n",
       "669              AccessHope\n",
       "670    ChaTeck Incorporated\n",
       "671           1-800-Flowers\n",
       "Name: Company Name, Length: 659, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Company Name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nicely done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let us look at the Salary Estimate column.  That will be a very useful column for analysis but in its current format it is a bit of a mess.  We can start by counting how many different salary ranges there are and also see what we will need to strip out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Salary Estimate\n",
       "$75K-$131K (Glassdoor est.)     32\n",
       "$79K-$131K (Glassdoor est.)     32\n",
       "$99K-$132K (Glassdoor est.)     32\n",
       "$137K-$171K (Glassdoor est.)    30\n",
       "$90K-$109K (Glassdoor est.)     28\n",
       "$56K-$97K (Glassdoor est.)      22\n",
       "$79K-$106K (Glassdoor est.)     22\n",
       "$90K-$124K (Glassdoor est.)     22\n",
       "$92K-$155K (Glassdoor est.)     21\n",
       "$138K-$158K (Glassdoor est.)    21\n",
       "$128K-$201K (Glassdoor est.)    21\n",
       "$212K-$331K (Glassdoor est.)    21\n",
       "$69K-$116K (Glassdoor est.)     21\n",
       "$124K-$198K (Glassdoor est.)    21\n",
       "$112K-$116K (Glassdoor est.)    21\n",
       "$91K-$150K (Glassdoor est.)     21\n",
       "$101K-$165K (Glassdoor est.)    21\n",
       "$110K-$163K (Glassdoor est.)    20\n",
       "$79K-$147K (Glassdoor est.)     20\n",
       "$145K-$225K(Employer est.)      20\n",
       "$31K-$56K (Glassdoor est.)      20\n",
       "$141K-$225K (Glassdoor est.)    20\n",
       "$66K-$112K (Glassdoor est.)     20\n",
       "$80K-$132K (Glassdoor est.)     20\n",
       "$87K-$141K (Glassdoor est.)     20\n",
       "$105K-$167K (Glassdoor est.)    20\n",
       "$79K-$133K (Glassdoor est.)     19\n",
       "$71K-$123K (Glassdoor est.)     19\n",
       "$122K-$146K (Glassdoor est.)    16\n",
       "$95K-$119K (Glassdoor est.)     16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Salary Estimate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our best bet here is to do a regex to strip out any of the non-numeric values, then create two columns with the minimum and maximum salaries for each row.  We can use the `str.extract()` function with a simple regex to only pull the first element in the salary range, which is the minimum salary.\n",
    "\n",
    "We can also multiply the salary number by 1000 since the salary range is in the thousands for a more accurate representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ds_df_reset = ds_df\n",
    "##ds_df = ds_df_reset\n",
    "##ds_df_reset = ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Min Salary'] = ds_df['Salary Estimate'].str.extract('\\$(\\d+)K').astype(float)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with the Maximum Salary, but this time pulling out the second element in the salary range to get the maximum salary.  We also multiply this number by 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Max Salary'] = ds_df['Salary Estimate'].str.extract(r'\\$(\\d+)K-\\$(\\d+)K')[1].astype(float)*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a few elements from each newly created column to confirm the transformations were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Min Salary</th>\n",
       "      <th>Max Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>137000.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>137000.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>137000.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>137000.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>137000.0</td>\n",
       "      <td>171000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>667</th>\n",
       "      <td>105000.0</td>\n",
       "      <td>167000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>668</th>\n",
       "      <td>105000.0</td>\n",
       "      <td>167000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>669</th>\n",
       "      <td>105000.0</td>\n",
       "      <td>167000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>670</th>\n",
       "      <td>105000.0</td>\n",
       "      <td>167000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>105000.0</td>\n",
       "      <td>167000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>659 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Min Salary  Max Salary\n",
       "0      137000.0    171000.0\n",
       "1      137000.0    171000.0\n",
       "2      137000.0    171000.0\n",
       "3      137000.0    171000.0\n",
       "4      137000.0    171000.0\n",
       "..          ...         ...\n",
       "667    105000.0    167000.0\n",
       "668    105000.0    167000.0\n",
       "669    105000.0    167000.0\n",
       "670    105000.0    167000.0\n",
       "671    105000.0    167000.0\n",
       "\n",
       "[659 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df[['Min Salary', 'Max Salary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better!  Now we can do analysis on the salary ranges more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have these columns in a better format for performing calculations, let us do a simple one to add an Average Salary column to the data set so we can slice by Minimum, Maximum, and Average Salary by job.\n",
    "\n",
    "To do this, we can use the `mean()` function on our two columns and specify `axis=1` so the calculation is peformed on every row.  If we do not make that specification in the function call, it will give us the average for all of the minimum and maximum salaries which is a good summary statistic but not our intention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Average Salary'] = ds_df[['Min Salary', 'Max Salary']].mean(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out a few sample values to confirm the average calculation performed on every row and looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      154000.0\n",
       "1      154000.0\n",
       "2      154000.0\n",
       "3      154000.0\n",
       "4      154000.0\n",
       "         ...   \n",
       "667    136000.0\n",
       "668    136000.0\n",
       "669    136000.0\n",
       "670    136000.0\n",
       "671    136000.0\n",
       "Name: Average Salary, Length: 659, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Average Salary']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have salary covered, let us look at the Size column by printing the value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Size\n",
       "51 to 200 employees        134\n",
       "1001 to 5000 employees     104\n",
       "1 to 50 employees           86\n",
       "201 to 500 employees        84\n",
       "10000+ employees            80\n",
       "501 to 1000 employees       77\n",
       "5001 to 10000 employees     61\n",
       "Unknown                     17\n",
       "-1                          16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Size'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that employees is repeated on every line and there are a few -1s that we should take care of.  Instead of leaving those as -1, we can transform them to \"Unknown\" because our intuition tells us that this was likely a blank value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Size'] = ds_df['Size'].str.replace('-1', 'Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will remove 'employees` from all of the entries so it looks a little better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Size'] = ds_df['Size'].str.split(' employees', expand=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the value counts now to see if the transformations were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Size\n",
       "51 to 200        134\n",
       "1001 to 5000     104\n",
       "1 to 50           86\n",
       "201 to 500        84\n",
       "10000+            80\n",
       "501 to 1000       77\n",
       "5001 to 10000     61\n",
       "Unknown           33\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Size'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks much better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the year the company was founded and use this to calculate how old the company is.  To start, we should see if there are any strange values in the Founded column by also looking value counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Founded\n",
       "-1       107\n",
       " 2012     34\n",
       " 2011     24\n",
       " 2010     22\n",
       " 1996     22\n",
       "        ... \n",
       " 1820      1\n",
       " 1952      1\n",
       " 1959      1\n",
       " 1894      1\n",
       " 1962      1\n",
       "Name: count, Length: 103, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Founded'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The -1 is back; because this column is an integer, and we want to use it to perform a simple calculation to subtract it from the current year of 2023, we can just replace it with a 0, then peform the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Founded'] = ds_df['Founded'].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the calculation correctly, we need to account for all of the companies that we set to 0.  So we can build a simple lambda function to say if the year = 0 then just set the age of the company to 0, otherwise subtract it from 2023 to get the number of years of the company.\n",
    "\n",
    "Lambda functions* are extremely important and powerful for data manipulation; the general syntax for performing a lambda function on a column is:\n",
    "\n",
    "`df.column.apply(lambda <perform this specific transformation to all elements in this list>`)\n",
    "\n",
    "*You can use these to do any sort of transformation you require that may not already exist in a library.  Learning how to build these is really helpful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Age of Company'] = ds_df['Founded'].apply(lambda x: 2023-x if x != 0 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results to see if the transformations worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age of Company\n",
       "0      107\n",
       "11      34\n",
       "12      24\n",
       "13      22\n",
       "27      22\n",
       "      ... \n",
       "203      1\n",
       "71       1\n",
       "64       1\n",
       "129      1\n",
       "61       1\n",
       "Name: count, Length: 103, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Age of Company'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!  There's a lot of young companies in this data set; perhaps data science still has some room for growth in the near future!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of time and brevity, we will perform a few more clean up items before we try visualizing the data.  Next up is taking a look at the job locations and seeing if there is any funky data there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location\n",
       "San Francisco, CA    58\n",
       "New York, NY         50\n",
       "Washington, DC       26\n",
       "Boston, MA           24\n",
       "Chicago, IL          22\n",
       "                     ..\n",
       "Lehi, UT              1\n",
       "Culver City, CA       1\n",
       "Lake Oswego, OR       1\n",
       "New Orleans, LA       1\n",
       "Irwindale, CA         1\n",
       "Name: count, Length: 207, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Location'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing major stands out here just yet.  So let's start by pulling out just the state abbrevations and disregarding the cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location\n",
       " CA              154\n",
       " VA               89\n",
       " MA               62\n",
       " NY               52\n",
       " MD               40\n",
       " IL               30\n",
       " DC               26\n",
       " TX               17\n",
       " WA               16\n",
       " OH               14\n",
       " PA               12\n",
       " MO               12\n",
       "United States     11\n",
       " NJ               10\n",
       " CO               10\n",
       " NC                9\n",
       " GA                9\n",
       " TN                8\n",
       " FL                8\n",
       " OK                6\n",
       " WI                6\n",
       "Remote             5\n",
       " IN                5\n",
       " MI                5\n",
       " CT                4\n",
       " AL                4\n",
       " MN                4\n",
       " AZ                4\n",
       " NE                3\n",
       " IA                3\n",
       " RI                2\n",
       "New Jersey         2\n",
       " SC                2\n",
       " OR                2\n",
       " UT                2\n",
       "Utah               2\n",
       " NH                2\n",
       " MS                1\n",
       " LA                1\n",
       " KS                1\n",
       "Texas              1\n",
       " DE                1\n",
       "California         1\n",
       " WV                1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Location'].apply(lambda x: x.split(\",\")[-1]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we see that a few of the job locations have the full state name or USA in them.  So let us create a simple function to deal with these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a state column so we have do not lose the original Location column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Job State'] = ds_df['Location'].apply(lambda x: x.split(\",\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could probably do this a little more programmatically if we had multiple states we had to change by using a simple mapping table, but for the purpose of this exercise we will keep it simple and just change the values to ones that seem appropriate.\n",
    "\n",
    "* California to CA\n",
    "* New Jersey to NJ\n",
    "* Texas to TX\n",
    "* Utah to UT\n",
    "* United States to USA\n",
    "* Remote to All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Job State'] = ds_df['Job State'].replace(['California', 'New Jersey', 'Texas', \n",
    "                                                 'Utah', 'United States', 'Remote'],\n",
    "                                                      ['CA', 'NJ', 'TX', 'UT', 'USA', 'All'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job State\n",
       "CA     155\n",
       "VA      89\n",
       "MA      62\n",
       "NY      52\n",
       "MD      40\n",
       "IL      30\n",
       "DC      26\n",
       "TX      18\n",
       "WA      16\n",
       "OH      14\n",
       "PA      12\n",
       "NJ      12\n",
       "MO      12\n",
       "USA     11\n",
       "CO      10\n",
       "GA       9\n",
       "NC       9\n",
       "FL       8\n",
       "TN       8\n",
       "OK       6\n",
       "WI       6\n",
       "MI       5\n",
       "All      5\n",
       "IN       5\n",
       "AZ       4\n",
       "CT       4\n",
       "MN       4\n",
       "AL       4\n",
       "UT       4\n",
       "IA       3\n",
       "NE       3\n",
       "SC       2\n",
       "OR       2\n",
       "RI       2\n",
       "NH       2\n",
       "MS       1\n",
       "LA       1\n",
       "KS       1\n",
       "DE       1\n",
       "WV       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Job State'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few more things to clean up and then we can save the data set, then do some quick visualizations to bring it all together.  Let's start by looking at the Job Titles in each of the descriptions and see if we can do some quick work on them.\n",
    "\n",
    "We start by looking at what the counts are for the Job Title column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job Title\n",
       "Data Scientist                                            326\n",
       "Data Engineer                                              26\n",
       "Senior Data Scientist                                      19\n",
       "Machine Learning Engineer                                  15\n",
       "Data Analyst                                               12\n",
       "                                                         ... \n",
       "Data Science Instructor                                     1\n",
       "Business Data Analyst                                       1\n",
       "Purification Scientist                                      1\n",
       "Data Engineer, Enterprise Analytics                         1\n",
       "AI/ML - Machine Learning Scientist, Siri Understanding      1\n",
       "Name: count, Length: 172, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Job Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purification Scientist sounds like a very interesting job title, but probably won't be very helpful for us to analyze.  It looks like the vast majority of the postings use the first five titles or some form of them.\n",
    "\n",
    "The best way to extract what the true job title for each listing is is to:\n",
    "\n",
    "* Set all of the characters in the job title string to lower case\n",
    "* Filter on the top five job titles\n",
    "* Set all others to Unknown Job Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we can create a super simple function and then apply it to the Job Title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def job_title_transformation(title):\n",
    "    title = title.lower()\n",
    "\n",
    "    if 'data scientist' in title:\n",
    "        return 'Data Scientist'\n",
    "    elif 'data analyst' in title:\n",
    "        return 'Data Analyst'\n",
    "    elif 'data engineer' in title:\n",
    "        return 'Data Engineer'\n",
    "    elif 'machine learning' in title:\n",
    "        return 'Machine Learning Engineer'\n",
    "    elif 'analyst' in title:\n",
    "        return 'Data Analyst'\n",
    "    else:\n",
    "        return 'Unknown Job Title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the function to the Job Title column and create a new column called Job Title Transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df['Job Title Transformed'] = ds_df['Job Title'].apply(job_title_transformation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm the new column looks as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Job Title Transformed\n",
       "Data Scientist               444\n",
       "Unknown Job Title             79\n",
       "Data Analyst                  55\n",
       "Data Engineer                 46\n",
       "Machine Learning Engineer     35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df['Job Title Transformed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!  We also see that there only 79 jobs that are in that miscellaneous bucket so it helps to bucket them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step in the transformation process: search of the most popular skills in the Job Description column.  To fit the theme of the book, we will pick some likely job skills and technologies that we presume we will see come up pretty often.\n",
    "\n",
    "* Python\n",
    "* Spark\n",
    "* SQL\n",
    "* Excel\n",
    "* Tableau\n",
    "* Power BI\n",
    "* AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this effectively, we will need to create boolean columns that indicate whether any of the skills shows up in the job description.  We can do this, once again, with a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flag_columns(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df['Job Description'].str.lower().str.contains(column).astype(int)\n",
    "\n",
    "columns = [\"python\", \"excel\", \"tableau\", \"power_bi\", \"aws\", \"sql\", \"spark\"]\n",
    "\n",
    "create_flag_columns(ds_df, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The create_flag_columns function takes two parameters: df (the DataFrame) & columns (a list of column names to create flags for.\n",
    "The function iterates over each column in the columns list.\n",
    "For each column, it converts the values in that column to lowercase using `str.lower()`, enabling case-insensitive comparisons.\n",
    "It then checks if the column name is present in the lowercase column values using `str.contains(column)`.\n",
    "The result of `str.contains()` is cast to int using `astype(int)`, converting True to 1 and False to 0.\n",
    "The resulting binary values are assigned to the respective columns in the DataFrame df.\n",
    "\n",
    "In the example usage, we pass the DataFrame ds_df and a list of column names columns to the create_flag_columns function. This will create flag columns for each specified column in ds_df, indicating whether the corresponding keyword is present in the respective column's values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "python\n",
       "1    479\n",
       "0    180\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.python.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Job Title', 'Salary Estimate', 'Job Description', 'Rating',\n",
       "       'Company Name', 'Location', 'Headquarters', 'Size', 'Founded',\n",
       "       'Type of ownership', 'Industry', 'Sector', 'Revenue', 'Competitors',\n",
       "       'Min Salary', 'Max Salary', 'Average Salary', 'Age of Company',\n",
       "       'Job State', 'Job Title Transformed', 'python', 'excel', 'tableau',\n",
       "       'power_bi', 'aws', 'sql', 'spark'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we applied various cleaning and transformation operations to specific columns. We replaced salary ranges with a unified format, extracted minimum and maximum salary values, cleaned and transformed the 'Rating', 'Size', 'Founded', 'Type of ownership', 'Industry', and 'Sector' columns using regular expressions and string manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Saving the Cleaned Dataset\n",
    "\n",
    "Finally, let's save the cleaned dataset to a new file for further analysis.\n",
    "\n",
    "By executing this code, we will save the cleaned dataset to a new CSV file called 'DS_Jobs_Final.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataset to a new CSV file\n",
    "ds_df.to_csv('DS_Jobs_Final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully completed the data cleaning process for the \"Data Science Job Postings\" dataset. The cleaned dataset can now be used for further analysis or machine learning tasks.\n",
    "\n",
    "Remember that data cleaning is an iterative process, and you can apply additional cleaning techniques based on the specific requirements of your analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
